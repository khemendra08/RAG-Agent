# RAG-Agent Project Documentation

## Project Overview

The RAG-Agent is a Retrieval-Augmented Generation system built using the crewAI framework. It allows users to search through PDF documents and the web for information on specific topics. The system uses a multi-agent approach where different AI agents collaborate to perform document analysis and generate comprehensive reports.

## Project Structure

```
RAG-Agent/
├── .env                   # Environment variables (OpenAI API key)
├── app.py                 # Streamlit web application
├── README.md              # Project documentation
├── README_STREAMLIT.md    # Streamlit-specific documentation
├── requirements.txt       # Project dependencies
├── run_test.py            # Script to test the crewAI functionality
├── report.md              # Output report generated by the agents
├── db/                    # Database directory for vector storage
│   └── chroma.sqlite3     # ChromaDB SQLite database for vector storage
└── src/                   # Source code
    └── pdf_rag/           # PDF RAG module
        ├── __init__.py    # Package initialization
        ├── crew.py        # CrewAI implementation
        ├── main.py        # Entry point for CLI execution
        ├── config/        # Configuration files
        │   ├── agents.yaml    # Agent definitions
        │   └── tasks.yaml     # Task definitions
        └── tools/         # Custom tools
            ├── __init__.py
            └── custom_tool.py # Example custom tool implementation
```

## Core Components

### 1. CrewAI Framework

The project is built on the [crewAI](https://crewai.com) framework, which enables the creation of multi-agent AI systems. The framework allows agents to collaborate on complex tasks, leveraging their collective intelligence and capabilities.

### 2. Agents

The system uses two main agents defined in `src/pdf_rag/config/agents.yaml`:

#### a. PDF Search Agent
- **Role**: Document and Web Search Specialist
- **Goal**: Search through PDF documents and the web to find relevant information
- **Tools**: PDFSearchTool and SerperDevTool
- **Functionality**: Extracts information from PDFs and searches the web for related content

#### b. Reporting Analyst
- **Role**: Topic-specific Reporting Analyst
- **Goal**: Create detailed reports based on document search findings and web research
- **Functionality**: Synthesizes information into well-structured reports with distinct sections

### 3. Tasks

Tasks are defined in `src/pdf_rag/config/tasks.yaml`:

#### a. PDF Research Task
- **Description**: Search through PDF documents and the web for information on a specific topic
- **Agent**: PDF Search Agent
- **Output**: List of relevant passages and information organized by source and relevance

#### b. Reporting Task
- **Description**: Synthesize findings into a comprehensive report
- **Agent**: Reporting Analyst
- **Output**: Detailed analytical report with sections for document-based findings, web-based information, and combined analysis

### 4. Tools

The system uses several tools to enhance agent capabilities:

#### a. PDFSearchTool
- Allows agents to search through PDF documents
- Extracts relevant information based on queries

#### b. SerperDevTool
- Enables web search functionality
- Retrieves up-to-date information from the internet

#### c. Custom Tool (Example)
- Demonstrates how to create custom tools for specific functionality

## Application Flow

### 1. CLI Flow (via main.py)

1. User runs the command `crewai run` from the project root
2. The `PdfRag().crew()` is initialized in `main.py`
3. The crew assembles agents and assigns tasks based on configuration
4. The PDF Search Agent searches the PDF document and web for information
5. The Reporting Analyst creates a comprehensive report
6. The output is saved to `report.md` in the project root

### 2. Web Application Flow (via app.py)

1. User uploads a PDF document through the Streamlit interface
2. User enters a search topic
3. When the "Run Search" button is clicked:
   - The PDF is saved to a temporary file
   - A `PdfRag().crew()` instance is created with the PDF path
   - The crew is kicked off with the search topic as input
4. Results are displayed in the web interface with the following layout:
   - PDF viewer at the top of the right column with a fixed height of 500px
   - Search results displayed directly below the PDF viewer
   - Clean interface with minimal white space between elements

## Key Files and Their Functions

### 1. crew.py

The core implementation of the CrewAI system:
- Defines the `PdfRag` class with the `@CrewBase` decorator
- Creates and configures agents with appropriate tools
- Defines tasks for the agents
- Assembles the crew with agents and tasks
- Sets up the sequential process flow

### 2. app.py

The Streamlit web application:
- Provides a user-friendly interface for uploading PDFs and entering search queries
- Configures the OpenAI model and other parameters
- Displays the PDF viewer and search results in an optimized layout
- Features a fixed-size PDF viewer that remains visible even when results are displayed
- Maintains a search history
- Uses custom CSS styling for an improved user experience

### 3. main.py

The CLI entry point:
- Provides functions for running, training, replaying, and testing the crew
- Sets up default inputs for the crew
- Handles command-line arguments for different operations

### 4. Configuration Files

#### a. agents.yaml
Defines the agents' roles, goals, and backstories, providing context for their behavior and expertise.

#### b. tasks.yaml
Specifies the tasks to be performed, including descriptions, expected outputs, and assigned agents.

## Data Flow

1. **Input**: 
   - PDF document (uploaded or specified by path)
   - Search topic (entered by user)

2. **Processing**:
   - PDF is analyzed using the PDFSearchTool
   - Web is searched using the SerperDevTool
   - Information is collected and organized by the PDF Search Agent
   - Reporting Analyst synthesizes the information into a comprehensive report

3. **Output**:
   - Detailed report with sections for document-based findings, web-based information, and combined analysis
   - Report is saved to `report.md` or displayed in the web interface

## Integration Points

1. **CrewAI Framework**:
   - The project leverages CrewAI for agent creation, task assignment, and process management

2. **OpenAI API**:
   - Used for language model capabilities (requires API key in `.env` file)

3. **Streamlit**:
   - Provides the web interface for user interaction

4. **ChromaDB**:
   - Used for vector storage and retrieval in the database directory

## Running the Project

### CLI Mode

```bash
crewai run
```

### Web Application Mode

```bash
streamlit run app.py
```

### Testing

```bash
python run_test.py
```
or
```bash
crewai test -n 5 -m gpt-4o-mini
```

## Customization Options

1. **Agents**: Modify `src/pdf_rag/config/agents.yaml` to define new agents or update existing ones
2. **Tasks**: Modify `src/pdf_rag/config/tasks.yaml` to define new tasks or update existing ones
3. **Tools**: Create custom tools in the `src/pdf_rag/tools` directory
4. **Models**: Select different OpenAI models in the web interface or modify the default in the code

## Dependencies

The project requires:
- Python >=3.10 <=3.13
- OpenAI API key
- CrewAI framework
- Streamlit for the web interface
- Various Python packages listed in `requirements.txt`
